{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpSQm8wIbFlcixW9UEO5zP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohith7livingston/Book-store/blob/main/ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik1ieiiDS-Z4"
      },
      "outputs": [],
      "source": [
        "#Exp - 1\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, avg, count, round, desc, when\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1Ô∏è‚É£ Create Spark Session\n",
        "spark = SparkSession.builder.appName(\"SalesDataAnalysis\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# 2Ô∏è‚É£ Load CSV Dataset\n",
        "df = spark.read.csv(\"sales_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "print(\"‚úÖ Original Data\")\n",
        "df.show()\n",
        "\n",
        "# -------------------------------\n",
        "# üîπ DATA OPERATIONS\n",
        "# -------------------------------\n",
        "\n",
        "# 3Ô∏è‚É£ Select specific columns\n",
        "selected_df = df.select(\"Product\", \"Category\", \"Region\", \"Sales\")\n",
        "print(\"\\nüîπ Selected Columns:\")\n",
        "selected_df.show()\n",
        "\n",
        "# 4Ô∏è‚É£ Filter rows (Sales > 10000)\n",
        "filtered_df = df.filter(df[\"Sales\"] > 10000)\n",
        "print(\"\\nüîπ Filtered Rows (Sales > 10000):\")\n",
        "filtered_df.show()\n",
        "\n",
        "# 5Ô∏è‚É£ Add a new column: total revenue = Sales * Quantity\n",
        "df = df.withColumn(\"Revenue\", df[\"Sales\"] * df[\"Quantity\"])\n",
        "print(\"\\nüîπ Added Revenue Column:\")\n",
        "df.show()\n",
        "\n",
        "# 6Ô∏è‚É£ Rename a column\n",
        "df = df.withColumnRenamed(\"Revenue\", \"Total_Revenue\")\n",
        "print(\"\\nüîπ Renamed Revenue Column:\")\n",
        "df.show()\n",
        "\n",
        "# 7Ô∏è‚É£ Sort data by Sales descending\n",
        "sorted_df = df.orderBy(df[\"Sales\"].desc())\n",
        "print(\"\\nüîπ Sorted by Sales (Descending):\")\n",
        "sorted_df.show()\n",
        "\n",
        "# -------------------------------\n",
        "# üîπ DATA ANALYSIS OPERATIONS\n",
        "# -------------------------------\n",
        "\n",
        "# 8Ô∏è‚É£ Group by Category and aggregate total + average sales\n",
        "category_agg = df.groupBy(\"Category\").agg(\n",
        "    sum(\"Sales\").alias(\"Total_Sales\"),\n",
        "    avg(\"Sales\").alias(\"Avg_Sales\")\n",
        ")\n",
        "print(\"\\nüìä Total & Average Sales by Category:\")\n",
        "category_agg.show()\n",
        "\n",
        "# 9Ô∏è‚É£ Group by Region: total quantity and average revenue\n",
        "region_stats = df.groupBy(\"Region\").agg(\n",
        "    sum(\"Quantity\").alias(\"Total_Quantity\"),\n",
        "    round(avg(\"Total_Revenue\"), 2).alias(\"Avg_Revenue\")\n",
        ")\n",
        "print(\"\\nüìä Quantity & Avg Revenue by Region:\")\n",
        "region_stats.show()\n",
        "\n",
        "# üîü Top 3 selling products by revenue\n",
        "top3 = df.orderBy(desc(\"Total_Revenue\")).limit(3)\n",
        "print(\"\\nüèÜ Top 3 Products by Revenue:\")\n",
        "top3.show()\n",
        "\n",
        "# 11Ô∏è‚É£ Descriptive statistics\n",
        "print(\"\\nüìà Summary Statistics:\")\n",
        "df.describe([\"Sales\", \"Quantity\", \"Total_Revenue\"]).show()\n",
        "\n",
        "# 12Ô∏è‚É£ Correlation between Sales and Quantity\n",
        "correlation = df.stat.corr(\"Sales\", \"Quantity\")\n",
        "print(f\"\\nüîó Correlation between Sales and Quantity: {correlation:.3f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# üîπ Visualization\n",
        "# -------------------------------\n",
        "cat_pd = category_agg.toPandas()\n",
        "region_pd = region_stats.toPandas()\n",
        "\n",
        "# Total Sales by Category\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(cat_pd[\"Category\"], cat_pd[\"Total_Sales\"], color='lightblue', edgecolor='black')\n",
        "plt.title(\"Total Sales by Category\")\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Total Sales\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Average Revenue by Region\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(region_pd[\"Region\"], region_pd[\"Avg_Revenue\"], color='orange', edgecolor='black')\n",
        "plt.title(\"Average Revenue by Region\")\n",
        "plt.xlabel(\"Region\")\n",
        "plt.ylabel(\"Average Revenue\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56678d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "7388346c-2f16-4a0e-9d00-fde7f674c29f"
      },
      "source": [
        "#exp2\n",
        "\n",
        "#box1\n",
        "import kagglehub\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#box2\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"Supermarket Sales by Gender Analysis\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "path = kagglehub.dataset_download(\"faresashraf1001/supermarket-sales\")\n",
        "csv_file_path = path\n",
        "\n",
        "df = spark.read.format(\"csv\") \\\n",
        ".option(\"header\", \"true\") \\\n",
        ".option(\"inferSchema\", \"true\") \\\n",
        ".load(csv_file_path)\n",
        "\n",
        "gender_group = df.groupBy(\"Gender\").count()\n",
        "gender_group.show()\n",
        "spark.stop()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character 'üìä' (U+1F4CA) (ipython-input-2928454268.py, line 142)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2928454268.py\"\u001b[0;36m, line \u001b[0;32m142\u001b[0m\n\u001b[0;31m    üìä 2Ô∏è‚É£ Data Analysis Operations\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'üìä' (U+1F4CA)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp - 3\n",
        "# Experiment 3 - Employee Data Analysis using PySpark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg, max, min, count\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1Ô∏è‚É£ Create Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Employee Data Analysis\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 2Ô∏è‚É£ Load JSON dataset\n",
        "file_path = \"employees.json\"\n",
        "df = spark.read.json(file_path)\n",
        "\n",
        "print(\"‚úÖ Employee Data Loaded Successfully\\n\")\n",
        "\n",
        "# 3Ô∏è‚É£ Display the dataset\n",
        "df.show()\n",
        "\n",
        "# 4Ô∏è‚É£ Display schema\n",
        "df.printSchema()\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ DATA MANIPULATION OPERATIONS\n",
        "# ------------------------------\n",
        "\n",
        "# 5Ô∏è‚É£ Filtering: employees with salary > 50000\n",
        "high_salary = df.filter(df[\"salary\"] > 50000)\n",
        "print(\"\\nüí∞ Employees with Salary > 50000:\")\n",
        "high_salary.show()\n",
        "\n",
        "# 6Ô∏è‚É£ Grouping: group by department and find average salary\n",
        "dept_avg_salary = df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
        "print(\"\\nüìä Average Salary by Department:\")\n",
        "dept_avg_salary.show()\n",
        "\n",
        "# 7Ô∏è‚É£ Sorting: sort by salary (descending)\n",
        "sorted_df = df.orderBy(df[\"salary\"].desc())\n",
        "print(\"\\n‚¨áÔ∏è Employees Sorted by Salary:\")\n",
        "sorted_df.show()\n",
        "\n",
        "# 8Ô∏è‚É£ Aggregation: count employees per department\n",
        "dept_count = df.groupBy(\"department\").agg(count(\"*\").alias(\"employee_count\"))\n",
        "print(\"\\nüë• Number of Employees per Department:\")\n",
        "dept_count.show()\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ VISUALIZATION\n",
        "# ------------------------------\n",
        "\n",
        "# Convert to Pandas for plotting\n",
        "dept_avg_pd = dept_avg_salary.toPandas()\n",
        "dept_count_pd = dept_count.toPandas()\n",
        "\n",
        "# --- Bar Chart: Average Salary by Department ---\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(dept_avg_pd[\"department\"], dept_avg_pd[\"avg_salary\"], color='skyblue', edgecolor='black')\n",
        "plt.title(\"Average Salary by Department\")\n",
        "plt.xlabel(\"Department\")\n",
        "plt.ylabel(\"Average Salary\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Pie Chart: Employee Count by Department ---\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.pie(\n",
        "    dept_count_pd[\"employee_count\"],\n",
        "    labels=dept_count_pd[\"department\"],\n",
        "    autopct=\"%1.1f%%\",\n",
        "    startangle=140,\n",
        "    colors=['lightcoral', 'lightgreen', 'lightskyblue', 'gold']\n",
        ")\n",
        "plt.title(\"Employee Distribution by Department\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ Stop Spark Session\n",
        "# ------------------------------\n",
        "spark.stop()\n",
        "print(\"\\n‚úÖ Spark Session stopped successfully.\")\n"
      ],
      "metadata": {
        "id": "Q1NCnh6QTC0D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exp4\n",
        "from pyspark.sql import SparkSession\n",
        "import matplotlib.pyplot as plt\n",
        "# Step 1: Create Spark Session\n",
        "spark = SparkSession.builder.appName(\"Spark SQL Operations\").getOrCreate()\n",
        "# Step 2: Create a DataFrame\n",
        "data = [\n",
        "(1, \"Lakshmi\", \"HR\", 50000),\n",
        "(2, \"Subbu\", \"IT\", 60000),\n",
        "(3, \"Gowri\", \"HR\", 55000),\n",
        "(4, \"Durga\", \"Finance\", 70000),\n",
        "(5, \"Ganesh\", \"IT\", 75000)\n",
        "]\n",
        "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "# Step 3: Register the DataFrame as a Temporary View\n",
        "df.createOrReplaceTempView(\"employees\")\n",
        "# Step 4: Write SQL Queries\n",
        "# 4.1 Select all records\n",
        "result_all = spark.sql(\"SELECT * FROM employees\")\n",
        "result_all.show()\n",
        "# 4.2 Filter employees with salary greater than 60000\n",
        "result_filter = spark.sql(\"SELECT * FROM employees WHERE salary > 60000\")\n",
        "result_filter.show()\n",
        "# 4.3 Group by department and calculate average salary\n",
        "result_group = spark.sql(\"SELECT department, AVG(salary) as avg_salary FROM employees GROUP BY department\")\n",
        "result_group.show()\n",
        "# 4.4 Order employees by salary in descending order\n",
        "result_order = spark.sql(\"SELECT * FROM employees ORDER BY salary DESC\")\n",
        "result_order.show()\n",
        "# 4.5 Find the maximum salary\n",
        "result_max = spark.sql(\"SELECT MAX(salary) as max_salary FROM employees\")\n",
        "result_max.show()\n",
        "# 4.6 Visualization: Average Salary by Department\n",
        "pandas_group = result_group.toPandas()\n",
        "plt.bar(pandas_group['department'], pandas_group['avg_salary'], color='purple')\n",
        "plt.xlabel('Department')\n",
        "plt.ylabel('Average Salary')\n",
        "plt.title('Average Salary by Department (SQL)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "# 4.7 Visualization: Employee Salary Distribution\n",
        "pandas_order = result_order.toPandas()\n",
        "plt.bar(pandas_order['name'], pandas_order['salary'], color='pink')\n",
        "plt.xlabel('Employee Name')\n",
        "plt.ylabel('Salary')\n",
        "plt.title('Employee Salary Distribution (SQL)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "# Optional: Stop the Spark Session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "Qnb2kM--EYP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Experiment 5 - Building a Data Pipeline with Apache Spark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, sum, count, round\n",
        "\n",
        "# 1Ô∏è‚É£ Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Sales Data Pipeline\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ----------------------------\n",
        "# üîπ STAGE 1: Extract (Read Raw Data)\n",
        "# ----------------------------\n",
        "\n",
        "# Example raw CSV dataset: \"sales_data.csv\"\n",
        "# Columns: id, product, category, region, sales, quantity\n",
        "\n",
        "data = [\n",
        "    (1, \"Laptop\", \"Electronics\", \"North\", 60000, 2),\n",
        "    (2, \"Mobile\", \"Electronics\", \"South\", 25000, 1),\n",
        "    (3, \"Chair\", \"Furniture\", \"East\", 8000, 3),\n",
        "    (4, \"Desk\", \"Furniture\", \"West\", 12000, 2),\n",
        "    (5, \"Headphones\", \"Electronics\", \"North\", 5000, 5),\n",
        "    (6, \"Sofa\", \"Furniture\", \"East\", 20000, 1),\n",
        "    (7, \"Monitor\", \"Electronics\", \"South\", 15000, 2),\n",
        "    (8, \"Table\", \"Furniture\", \"North\", 9000, 4)\n",
        "]\n",
        "\n",
        "columns = [\"id\", \"product\", \"category\", \"region\", \"sales\", \"quantity\"]\n",
        "\n",
        "df_raw = spark.createDataFrame(data, columns)\n",
        "print(\"‚úÖ Raw Data Loaded (Extract Stage):\")\n",
        "df_raw.show()\n",
        "\n",
        "# ----------------------------\n",
        "# üîπ STAGE 2: Transform (Clean & Process)\n",
        "# ----------------------------\n",
        "\n",
        "# Example transformations:\n",
        "#  - Remove records with null values\n",
        "#  - Filter high-value transactions (sales > 10000)\n",
        "#  - Compute average and total sales by category and region\n",
        "\n",
        "# Clean data\n",
        "df_clean = df_raw.na.drop()\n",
        "\n",
        "# Filter\n",
        "df_filtered = df_clean.filter(df_clean[\"sales\"] > 10000)\n",
        "\n",
        "# Aggregate\n",
        "df_summary = df_filtered.groupBy(\"category\", \"region\").agg(\n",
        "    sum(\"sales\").alias(\"total_sales\"),\n",
        "    round(avg(\"sales\"), 2).alias(\"avg_sales\"),\n",
        "    count(\"*\").alias(\"transaction_count\")\n",
        ")\n",
        "\n",
        "print(\"\\nüìä Transformed Data (Transform Stage):\")\n",
        "df_summary.show()\n",
        "\n",
        "# ----------------------------\n",
        "# üîπ STAGE 3: Load (Save Processed Data)\n",
        "# ----------------------------\n",
        "\n",
        "output_path = \"output/sales_summary\"\n",
        "df_summary.write.mode(\"overwrite\").csv(output_path, header=True)\n",
        "\n",
        "print(f\"\\n‚úÖ Processed data written to: {output_path}\")\n",
        "\n",
        "# ----------------------------\n",
        "# üîπ Stop Spark\n",
        "# ----------------------------\n",
        "spark.stop()\n",
        "print(\"\\n‚úÖ Data pipeline executed successfully.\")\n"
      ],
      "metadata": {
        "id": "M9Y9WLfNEg6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exp6\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# 1Ô∏è‚É£ Create Spark session\n",
        "spark = SparkSession.builder.appName(\"SupermarketSalesSQL\").getOrCreate()\n",
        "\n",
        "# 2Ô∏è‚É£ Read CSV file\n",
        "df = spark.read.csv(\"/content/supermarket_sales.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# 3Ô∏è‚É£ Display schema and first rows\n",
        "print(\"üìÑ Schema of dataset:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\nüìÑ First 10 rows from CSV:\")\n",
        "df.show(10)\n",
        "\n",
        "# 4Ô∏è‚É£ Save as Parquet\n",
        "df.write.mode(\"overwrite\").parquet(\"supermarket_sales_parquet\")\n",
        "\n",
        "# 5Ô∏è‚É£ Register as SQL table\n",
        "df.createOrReplaceTempView(\"sales\")\n",
        "\n",
        "# 6Ô∏è‚É£ SQL Query 1: Total Sales per Product line\n",
        "print(\"\\nüìä Total Sales per Product line:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT `Product line`, ROUND(SUM(Total), 2) AS Total_Sales\n",
        "    FROM sales\n",
        "    GROUP BY `Product line`\n",
        "    ORDER BY Total_Sales DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# 7Ô∏è‚É£ SQL Query 2: Average Sales per Branch (since no gross income column)\n",
        "print(\"\\nüè¢ Average Sales per Branch:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT Branch, ROUND(AVG(Total), 2) AS Avg_Sales\n",
        "    FROM sales\n",
        "    GROUP BY Branch\n",
        "    ORDER BY Avg_Sales DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# 8Ô∏è‚É£ SQL Query 3: Number of Transactions by City (instead of Payment)\n",
        "print(\"\\nüåÜ Number of Transactions by City:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT City, COUNT(*) AS Transactions\n",
        "    FROM sales\n",
        "    GROUP BY City\n",
        "    ORDER BY Transactions DESC\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "id": "mHaRbPa6EmUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Experiment 7 - Developing a Parquet Table into a Data Platform Container\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, sum, count, round\n",
        "\n",
        "# 1Ô∏è‚É£ Create Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Develop Parquet Table to Data Container\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 1: Create Sample Data\n",
        "# -----------------------------\n",
        "data = [\n",
        "    (1, \"Laptop\", \"Electronics\", \"North\", 60000, 2),\n",
        "    (2, \"Mobile\", \"Electronics\", \"South\", 25000, 1),\n",
        "    (3, \"Chair\", \"Furniture\", \"East\", 8000, 3),\n",
        "    (4, \"Desk\", \"Furniture\", \"West\", 12000, 2),\n",
        "    (5, \"Headphones\", \"Electronics\", \"North\", 5000, 5),\n",
        "    (6, \"Sofa\", \"Furniture\", \"East\", 20000, 1),\n",
        "    (7, \"Monitor\", \"Electronics\", \"South\", 15000, 2),\n",
        "    (8, \"Table\", \"Furniture\", \"North\", 9000, 4)\n",
        "]\n",
        "columns = [\"id\", \"product\", \"category\", \"region\", \"sales\", \"quantity\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "print(\"‚úÖ Sample DataFrame Created:\\n\")\n",
        "df.show()\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 2: Transform the Data\n",
        "# -----------------------------\n",
        "df_summary = df.groupBy(\"category\", \"region\").agg(\n",
        "    round(avg(\"sales\"), 2).alias(\"avg_sales\"),\n",
        "    sum(\"sales\").alias(\"total_sales\"),\n",
        "    count(\"*\").alias(\"transactions\")\n",
        ")\n",
        "\n",
        "print(\"\\nüìä Aggregated Sales Summary:\")\n",
        "df_summary.show()\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 3: Define Data Container Path (Local or Cloud)\n",
        "# -----------------------------\n",
        "\n",
        "# üóÇÔ∏è Local data container (you can replace this path with S3, ADLS, etc.)\n",
        "data_container_path = \"data_container/sales_data/\"\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 4: Write DataFrame as Parquet Table into the Container\n",
        "# -----------------------------\n",
        "df_summary.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"category\") \\\n",
        "    .parquet(data_container_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Parquet Table successfully written to Data Container: {data_container_path}\")\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 5: Read from the Parquet Container (Verify)\n",
        "# -----------------------------\n",
        "print(\"\\nüì¶ Reading Parquet Table from Data Container:\")\n",
        "parquet_df = spark.read.parquet(data_container_path)\n",
        "parquet_df.show()\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 6: Register as SQL Table\n",
        "# -----------------------------\n",
        "parquet_df.createOrReplaceTempView(\"sales_summary\")\n",
        "\n",
        "print(\"\\nüß† Running SQL Query on Container Table:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT category, region, total_sales\n",
        "    FROM sales_summary\n",
        "    WHERE total_sales > 20000\n",
        "    ORDER BY total_sales DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 7: Stop Spark Session\n",
        "# -----------------------------\n",
        "spark.stop()\n",
        "print(\"\\n‚úÖ Spark Session stopped successfully.\")\n"
      ],
      "metadata": {
        "id": "FietV2iXEmO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Experiment 8 - Running SQL Queries on Data in a NoSQL (MongoDB) Table\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# 1Ô∏è‚É£ Create Spark Session with MongoDB Connector\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Run SQL Queries on NoSQL Table\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://127.0.0.1:27017/company.employees\") \\\n",
        "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://127.0.0.1:27017/company.output\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 2: Read Data from MongoDB Collection\n",
        "# -----------------------------\n",
        "df = spark.read.format(\"mongo\").load()\n",
        "\n",
        "print(\"‚úÖ Data successfully loaded from MongoDB collection 'employees':\")\n",
        "df.show()\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 3: Register as Temporary View for SQL Queries\n",
        "# -----------------------------\n",
        "df.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 4: Run SQL Queries\n",
        "# -----------------------------\n",
        "\n",
        "# üü¢ Query 1: Show All Employees\n",
        "print(\"\\nüü¢ Query 1: SELECT * FROM employees\")\n",
        "spark.sql(\"SELECT * FROM employees\").show()\n",
        "\n",
        "# üü¢ Query 2: Employees with salary > 50000\n",
        "print(\"\\nüü¢ Query 2: Employees with Salary > 50000\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT name, department, salary\n",
        "    FROM employees\n",
        "    WHERE salary > 50000\n",
        "    ORDER BY salary DESC\n",
        "\"\"\").show()\n",
        "\n",
        "# üü¢ Query 3: Average Salary by Department\n",
        "print(\"\\nüü¢ Query 3: Average Salary by Department\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT department, ROUND(AVG(salary), 2) AS avg_salary\n",
        "    FROM employees\n",
        "    GROUP BY department\n",
        "\"\"\").show()\n",
        "\n",
        "# üü¢ Query 4: Count Employees per Department\n",
        "print(\"\\nüü¢ Query 4: Count of Employees by Department\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT department, COUNT(*) AS emp_count\n",
        "    FROM employees\n",
        "    GROUP BY department\n",
        "\"\"\").show()\n",
        "\n",
        "# -----------------------------\n",
        "# üîπ Step 5: Stop Spark Session\n",
        "# -----------------------------\n",
        "spark.stop()\n",
        "print(\"\\n‚úÖ Spark Session stopped successfully.\")\n"
      ],
      "metadata": {
        "id": "BPEeIy8ZEvhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp - 9\n",
        "from pyspark.sql import SparkSession\n",
        "from delta.tables import DeltaTable\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# 1Ô∏è‚É£ Create Spark Session with Delta Lake Support\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Modify Delta Lake Table\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 2Ô∏è‚É£ Define Delta Table Path\n",
        "path = \"delta_table/employees\"\n",
        "\n",
        "# 3Ô∏è‚É£ Create Sample Table (if not exists)\n",
        "data = [\n",
        "    (1, \"Alice\", \"HR\", 45000),\n",
        "    (2, \"Bob\", \"IT\", 75000),\n",
        "    (3, \"Charlie\", \"Finance\", 55000)\n",
        "]\n",
        "cols = [\"id\", \"name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, cols)\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
        "print(\"‚úÖ Delta Table Created Successfully\")\n",
        "\n",
        "# 4Ô∏è‚É£ Load Existing Delta Table\n",
        "deltaTable = DeltaTable.forPath(spark, path)\n",
        "print(\"\\nüìÑ Original Table:\")\n",
        "deltaTable.toDF().show()\n",
        "\n",
        "# 5Ô∏è‚É£ Update: Increase salary by 10% for IT dept\n",
        "deltaTable.update(\n",
        "    condition=col(\"department\") == \"IT\",\n",
        "    set={\"salary\": col(\"salary\") * 1.10}\n",
        ")\n",
        "\n",
        "# 6Ô∏è‚É£ Insert: Add a new employee record\n",
        "new_data = [(4, \"David\", \"Sales\", 50000)]\n",
        "new_df = spark.createDataFrame(new_data, cols)\n",
        "new_df.write.format(\"delta\").mode(\"append\").save(path)\n",
        "\n",
        "# 7Ô∏è‚É£ Delete: Remove employees with salary < 48000\n",
        "deltaTable.delete(condition=col(\"salary\") < 48000)\n",
        "\n",
        "# 8Ô∏è‚É£ Show Final Updated Table\n",
        "print(\"\\n‚úÖ Updated Delta Table:\")\n",
        "deltaTable.toDF().show()\n",
        "\n",
        "# 9Ô∏è‚É£ Stop Spark Session\n",
        "spark.stop()\n",
        "print(\"\\n‚úÖ Spark Session Stopped Successfully.\")\n"
      ],
      "metadata": {
        "id": "9J2Ozt76QZlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta.tables import DeltaTable\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DeltaLakeExample\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create sample data\n",
        "data = [(1, \"Alice\", 45000), (2, \"Bob\", 55000)]\n",
        "cols = [\"id\", \"name\", \"salary\"]\n",
        "df = spark.createDataFrame(data, cols)\n",
        "\n",
        "# Write as Delta table\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(\"delta_table/employees\")\n",
        "\n",
        "# Read Delta table\n",
        "df_read = spark.read.format(\"delta\").load(\"delta_table/employees\")\n",
        "df_read.show()\n",
        "\n",
        "# Update Delta table\n",
        "deltaTable = DeltaTable.forPath(spark, \"delta_table/employees\")\n",
        "deltaTable.update(col(\"name\") == \"Alice\", {\"salary\": col(\"salary\") + 5000})\n",
        "\n",
        "# Show updated data\n",
        "deltaTable.toDF().show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "0oxGKUjzRHOK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}